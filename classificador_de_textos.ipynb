{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classificador_de_textos.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2y7hRcB0IZTa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBSC4RUov61i"
      },
      "source": [
        "# **Projeto NLP - Classificador de Textos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y7hRcB0IZTa"
      },
      "source": [
        "## **1 - Tarefa e Dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKM8TzuQOWJr"
      },
      "source": [
        "##### **Base de Dados**\r\n",
        "\r\n",
        "TweetSentBR é um corpus de Tweets em Português do Brasil. Ele foi rotulado por vários anotadores seguindo etapas estabelecidas na literatura para melhorar a confiabilidade na tarefa de Análise de Sentimento. Cada tweet foi anotado em uma das três classes a seguir:\r\n",
        "\r\n",
        "*   **Positivo** - tweets em que o usuário quis dizer uma reação \r\n",
        "positiva ou avaliação sobre o tópico principal da postagem;\r\n",
        "*   **Negativo** - tweets em que o usuário quis dizer uma reação negativa ou avaliação sobre o tópico principal da postagem;\r\n",
        "*   **Neutro** - tweets que não pertencem a nenhuma das últimas classes, geralmente não fazem ponto, fora do tópico, irrelevantes, confusos ou contendo apenas dados objetivos.\r\n",
        "\r\n",
        "**Link:** https://bitbucket.org/HBrum/tweetsentbr/src/master/ \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpDV4XitS_6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5ac20a-a21e-433b-b2da-03ecc3cc5f14"
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuEXwrHOMGsu"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import wordnet as wn\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from spellchecker import SpellChecker\n",
        "from google.colab import drive"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-afMkP5FBP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6adf2d68-5605-402e-beee-31b6a466a26a"
      },
      "source": [
        "spacy.cli.download('pt_core_news_sm')"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuTgo37_X-3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a21126-cdab-41b0-fd46-96f97d04c5f3"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw')"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1fH0TKYlaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5092ac20-5666-41d5-8b13-453171b85dc3"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "path = 'drive/My Drive/AS/'"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4beTlS0yXydp"
      },
      "source": [
        "##### **Expansão da lista de palavras com polaridade**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxfY2wnshvFy"
      },
      "source": [
        "def search_synonyms(word):\n",
        "  synonyms = []\n",
        "  #antonyms = []\n",
        "  for synonym in wn.synsets(word, lang=('por')):\n",
        "    for lemma in synonym.lemmas('por'):\n",
        "      synonyms.append(lemma.name())\n",
        "      #if lemma.antonyms():\n",
        "        #if lemma.antonyms()[0].name() != word:\n",
        "          #antonyms.append(lemma.antonyms()[0].name())\n",
        "  return synonyms"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5eMDRGkqnyC"
      },
      "source": [
        "def insert_word(positive, negative, word, polarity, new_words):\n",
        "  if((word not in positive) and (word not in negative)):\n",
        "    if(polarity == 1):\n",
        "      positive.append(word)\n",
        "    else:\n",
        "      negative.append(word)\n",
        "    new_words.append(word)\n",
        "  return positive, negative, new_words"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRTeoc7qzBo-"
      },
      "source": [
        "def read_seeds(filename):\n",
        "  df = pd.read_csv(path + filename, delimiter=';')  # leitura do arquivo\n",
        "  lines = df.loc[:].values\n",
        "  negative = []\n",
        "  positive = []\n",
        "  new_words = []\n",
        "  for line in lines:\n",
        "    word = line[0]\n",
        "    polarity = line[1]\n",
        "    if(polarity == 1):\n",
        "      positive.append(word)\n",
        "    else:\n",
        "      negative.append(word)\n",
        "    synonyms = search_synonyms(word)\n",
        "    for synonym in synonyms:\n",
        "      positive, negative, new_words = insert_word(positive, negative, synonym,\n",
        "                                                  polarity, new_words)\n",
        "  return positive, negative, new_words"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csNqZNF1apeB"
      },
      "source": [
        "def expand_seeds(positive, negative, words):\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    if word in positive:\n",
        "      polarity = 1\n",
        "    else:\n",
        "      polarity = -1\n",
        "    synonyms = search_synonyms(word)\n",
        "    for synonym in synonyms:\n",
        "        positive, negative, new_words = insert_word(positive, negative, synonym,\n",
        "                                                    polarity, new_words)\n",
        "  while(len(new_words) > 0):\n",
        "    print(len(new_words))\n",
        "    print(new_words)\n",
        "    positive, negative, new_words = expand_seeds(positive, negative, new_words)\n",
        "  return positive, negative, new_words\n"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo_GwGu0VzWP"
      },
      "source": [
        "def read_verbs(filename, positive, negative):\n",
        "  df = pd.read_csv(path + filename, delimiter=';')  # leitura do arquivo\n",
        "  lines = df.loc[:].values\n",
        "  for line in lines:\n",
        "    word = line[0]\n",
        "    polarity = line[1]\n",
        "    if(polarity == 1):\n",
        "      positive.append(word)\n",
        "    else:\n",
        "      negative.append(word)\n",
        "  return positive, negative"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLmUyv5qQuMs"
      },
      "source": [
        "##### **Leitura do corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbfbSqKnSsc-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8yZLXX2Q1QF"
      },
      "source": [
        "def read_corpus(filename):\n",
        "  corpus = pd.read_csv(path + filename, delimiter='\\t')\n",
        "  #document = corpus.iloc[:,3]\n",
        "  #polarity = corpus.iloc[:,1]\n",
        "  document = corpus.iloc[:,0]\n",
        "  polarity = corpus.iloc[:,1]\n",
        "  document, document_, polarity, polarity_ = train_test_split(document, polarity, test_size=0.15, random_state=0)\n",
        "  return corpus, document_, polarity_"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU5QmoCOKj9W"
      },
      "source": [
        "##### **Separação do documento em frases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLfeTQ5TKl3I"
      },
      "source": [
        "def search_phrases(document):\n",
        "  str = document\n",
        "  x = re.search(\"[?][\\s]|[!][\\s]|[.][\\s]\", str)\n",
        "  if(x):\n",
        "    phrases = re.split(\"[?][\\s]|[!][\\s]|[.][\\s]\", str)\n",
        "  else:\n",
        "    phrases = [document]\n",
        "  return phrases"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfSqWQiMACsr"
      },
      "source": [
        "##### **Deleta entidades nomeadas (REN) da frase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeT83fSSHbgM"
      },
      "source": [
        "def delete_entities(phrase):\n",
        "  nlp = spacy.load(\"pt_core_news_sm\")\n",
        "  doc = nlp(phrase)\n",
        "  for entity in doc.ents:\n",
        "    phrase = phrase.replace(entity.text, '')\n",
        "  return phrase"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKsN0ObF_GMi"
      },
      "source": [
        "##### **Tokenização e POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyZuX-V79cN0"
      },
      "source": [
        "def pos_tagging(phrase):\n",
        "  token_list = []\n",
        "  token_with_emoji = []\n",
        "  pos_list = []\n",
        "  classes = ['ADV', 'ADJ', 'VERB', 'NOUN', 'SCONJ', 'ADP', 'PRON', 'PROPN', 'SYM', 'X']\n",
        "  nlp = spacy.load(\"pt_core_news_sm\")\n",
        "  doc = nlp(phrase)\n",
        "  for token in doc:\n",
        "    if token.pos_ in classes or token.text == 'nem':\n",
        "      token_list.append(token.text)\n",
        "      pos_list.append(token.pos_)\n",
        "    token_with_emoji.append(token.text)  \n",
        "  return token_list, pos_list, token_with_emoji"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqqO_gYHQO78"
      },
      "source": [
        "## **2 - Visualização dos dados**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJvsBOB5CG_C"
      },
      "source": [
        "##### **Correção ortográfica**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz3JEheV05w8"
      },
      "source": [
        "def spell_checker(token_list):\n",
        "  spell = SpellChecker(language='pt')\n",
        "  misspelled = spell.unknown(token_list)\n",
        "  for word in misspelled:\n",
        "    try:\n",
        "      index = token_list.index(word)\n",
        "      del(token_list[index])\n",
        "      token_list.insert(index, spell.correction(word))\n",
        "    except:\n",
        "      print(\"Não encontrado na lista!\")\n",
        "    #spell.candidates(word) # lista de opções possíveis de correção\n",
        "  return token_list"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbjJNEA8InJr"
      },
      "source": [
        "##### **Conversão para caracteres minúsculos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exYYoxh_Ixzx"
      },
      "source": [
        "def to_lower(token_list):\n",
        "  new_token_list = []\n",
        "  for token in token_list:\n",
        "    new_token_list.append(token.lower())\n",
        "  return new_token_list"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYkcqG2w8O9P"
      },
      "source": [
        "##### **Lematização**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Sz0Yl28OWu"
      },
      "source": [
        "def lemmatization(phrase):\n",
        "  lemma_list = []\n",
        "  nlp = spacy.load(\"pt_core_news_sm\")\n",
        "  doc = nlp(phrase)\n",
        "  for token in doc:\n",
        "    lemma_list.append(token.lemma_)\n",
        "  return lemma_list"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWnZtgwj1j1n"
      },
      "source": [
        "## **3 - Classificadores**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqN63Gl67kkS"
      },
      "source": [
        "##### **Risada**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozXlS_K4iYR9"
      },
      "source": [
        "def check_laugh(token_list, positive):\n",
        "  for token in token_list:\n",
        "    str = token\n",
        "    x = re.search(\"([hH][aeiouAEIOU])([hH][aeiouAEIOU])+|[k][k]+|[K][K]+|[e][e]+|[z][z]+|[Z][Z]+|[uU][hH][uU][lL]+\", str)\n",
        "    if(x):\n",
        "      positive += 1\n",
        "  return positive"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipGzw8I6QHEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f12fac-dab0-4c27-d00e-42796f307cec"
      },
      "source": [
        "print(check_laugh(['cláudia', 'raia', 'hehe', 'aha', 'kaleu', 'kj'],0))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajKPURiFEGB"
      },
      "source": [
        "##### **Busca de advérbio de negação**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YffLzUbFx4v"
      },
      "source": [
        "def search_negative_word(token_list, pos_list):\n",
        "  negative_words = ['não', 'nunca', 'jamais', 'sem','ninguém','sqn']\n",
        "  for negative_word in negative_words:\n",
        "    if negative_word in token_list:\n",
        "      index = token_list.index(negative_word)\n",
        "      try:\n",
        "        if ((pos_list[index+1] == 'VERB' or pos_list[index+1] == 'ADJ' \n",
        "          or pos_list[index+1] == 'NOUN') and (token_list.count(\n",
        "              negative_word) == 1)):\n",
        "          return \"found\", index\n",
        "      except:\n",
        "        print(\"Fora do índice\")\n",
        "  return \"not found\", -1000"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXVwXSz_-_2_"
      },
      "source": [
        "##### **Busca de sentimento na frase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvkq-_SG_EnR"
      },
      "source": [
        "def search_sentiment(token_list, pos_list, positive, negative):\n",
        "  positive_word = 0\n",
        "  negative_word = 0\n",
        "  result, index = search_negative_word(token_list, pos_list)\n",
        "  for token in token_list:\n",
        "    if((result == \"not found\") or ((token_list.index(token) != index+1))):\n",
        "      if token in positive:\n",
        "        positive_word += 1\n",
        "      elif token in negative:\n",
        "        negative_word += 1\n",
        "    elif((result == \"found\") and ((token_list.index(token) == index+1))):\n",
        "      if token in positive:\n",
        "        negative_word += 1\n",
        "      elif token in negative:\n",
        "        positive_word += 1\n",
        "  return positive_word, negative_word"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9pN6VAVOC7O"
      },
      "source": [
        "##### **Busca de emojis na frase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66LXxiD94a7n"
      },
      "source": [
        "def search_emojis(token_with_emoji):\n",
        "  positive = 0\n",
        "  negative = 0\n",
        "  positive_words = ['homão','mulherão','lacradora','crush','crushs','+','ícone',\n",
        "                    'bff', 'mito', 'memes','ótimo','obrigado',\n",
        "                    'xuxu','migo','fofo','deusa','deuso','top']\n",
        "  negative_words = ['pqp','sdds','poxa','embuste', 'aff', 'bad', 'sad','sozinho']\n",
        "  positive_emoji = ['😀', '😬', '😁', '😂', '😃', '😄', '🤣', '😆', '😇',\n",
        "                    '😉', '😊', '🙂', '🌟','🐓', '🍝', '🌈','🦄 ', '😋',\n",
        "                    '😌', '😍', '😘', '😗', '😙', '😚', '🤪', '😜', '😝',\n",
        "                    '😛', '🤑', '😎', '🤜', '🍜', '🤘','😻', '🤓', '🧐',\n",
        "                    '🤠', '🤗', '🤲', '🙌', '👏', '🙏', '🤝', '👍', '✌',\n",
        "                    '👊', '♥', '❤','💔',':)','<3']\n",
        "  negative_emoji = ['🤡', '😏', '😶', '😐', '😑', '😒', '🙄', '🤨', '🤔',\n",
        "                    '🤫', '🤭', '🤥', '😳', '😞', '😟', '😠', '😡', '🤬',\n",
        "                    '😔', '😕', '🙁', '😣', '😖', '😫', '😩', '😤', '😅',\n",
        "                   '😮', '😱', '😨', '😰', '😯', '😦', '😧', '😢', '😥',\n",
        "                    '😪', '🤤', '😓', '😭', ':(','o.O',':O' ]\n",
        "  for token in token_with_emoji:\n",
        "    if token in positive_emoji:\n",
        "      positive += 1\n",
        "    elif token in negative_emoji:\n",
        "      negative += 1\n",
        "  token_with_emoji = to_lower(token_with_emoji)\n",
        "  for token in token_with_emoji:\n",
        "    if token in positive_words:\n",
        "      positive += 1\n",
        "    elif token in negative_words:\n",
        "      negative += 1\n",
        "  return positive, negative"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6vCA0cNHWl8"
      },
      "source": [
        "##### **Definição da análise de sentimento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bFE2GWnOUfR"
      },
      "source": [
        "def sentiment_analysis(positive_emoji, negative_emoji, \n",
        "                       positive_word, negative_word):\n",
        "  if(positive_word == 0 and negative_word == 0 and\n",
        "     positive_emoji == 0 and negative_emoji == 0):\n",
        "    result = 0 # neutral\n",
        "    #result = \"NE\"\n",
        "  elif(positive_word == negative_word and positive_emoji == negative_emoji):\n",
        "    result = -1 # positive\n",
        "    #result = \"PO\"\n",
        "  else:\n",
        "    positive = positive_word + positive_emoji\n",
        "    negative = negative_word + negative_emoji\n",
        "    if(positive <= negative):\n",
        "      if(positive_emoji == negative_word and (\n",
        "          positive_word == negative_emoji and positive_emoji > 0)):\n",
        "        result = 1 #positive\n",
        "        #result = \"PO\"\n",
        "      else:\n",
        "        result = -1 # negative\n",
        "        #result = \"NG\"\n",
        "    else:\n",
        "      result = 1 # positive\n",
        "      #result = \"PO\"\n",
        "  return result"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGOVQg_5i5QO"
      },
      "source": [
        "## **4 - Resultados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOw6y78drz2"
      },
      "source": [
        "positive, negative, new_words = read_seeds('sementes.csv')"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCc_1Pw3vlZ6"
      },
      "source": [
        "positive = lemmatization(' '.join(map(str, positive)))\n",
        "negative = lemmatization(' '.join(map(str, negative)))\n",
        "positive, negative = read_verbs('verbos.csv', positive, negative)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe809xJfWRHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a665f5-3bac-4946-ab2c-2250793750a1"
      },
      "source": [
        "print(len(positive))\n",
        "print(len(negative))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "520\n",
            "538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QLj40KT7Mbp"
      },
      "source": [
        "def delete_repeated(words):\n",
        "    lemmas = []\n",
        "    [ lemmas.append(item) for item in words if not lemmas.count(item) ]\n",
        "    return lemmas"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leIkgTQYvqOq"
      },
      "source": [
        "corpus, document, polarity = read_corpus('TweetSentBR.txt')"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gMsfXoq0eHX"
      },
      "source": [
        "results = []\n",
        "positive_word = 0\n",
        "negative_word = 0\n",
        "positive_emoji = 0\n",
        "negative_emoji = 0\n",
        "count = 0\n",
        "for doc in document:\n",
        "  phrase = doc\n",
        "  token, pos, token_with_emoji = pos_tagging(phrase)\n",
        "  positive_word = check_laugh(token, positive_word)\n",
        "  token = to_lower(spell_checker(token))\n",
        "  lemma = lemmatization(' '.join(map(str, token)))\n",
        "  i, j = search_sentiment(lemma, pos, positive, negative)\n",
        "  positive_word += i\n",
        "  negative_word += j\n",
        "  i, j = search_emojis(token_with_emoji)\n",
        "  positive_emoji += i\n",
        "  negative_emoji += j\n",
        "  result = sentiment_analysis(positive_emoji, negative_emoji,\n",
        "                                positive_word, negative_word)\n",
        "  results.append(result)\n",
        "  token.clear()\n",
        "  pos.clear()\n",
        "  positive_word = 0\n",
        "  negative_word = 0\n",
        "  positive_emoji = 0\n",
        "  negative_emoji = 0\n",
        "  lemma.clear()\n",
        "  count += 1\n",
        "  if(count%10==0 or count == 1):\n",
        "    print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StJCmPBbQIz6"
      },
      "source": [
        "final = pd.DataFrame(columns=['doc','pol','result'])\n",
        "final['doc'] = document\n",
        "final['pol'] = polarity\n",
        "final['result'] = results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu3biUbCftHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "3465d5b9-8ce2-48d2-9153-30e7daf95259"
      },
      "source": [
        "final"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>pol</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1670</th>\n",
              "      <td>esse homem é uma perdição 😍 😈 USERNAME USERNAME</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13379</th>\n",
              "      <td>que fome da porra aí eu invento de ir assistir</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10234</th>\n",
              "      <td>boa noite ! 📺</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4719</th>\n",
              "      <td>A moça rindo 😱 😱 😱 😂 😂 😂 😂</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7003</th>\n",
              "      <td>USERNAME agora que o bicho vai pegar 😂 😂 😂 😂 😂...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3943</th>\n",
              "      <td>o filho do serginho ❤ ️ ❤ ️ ❤ ️ to apaixonadaa...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3183</th>\n",
              "      <td>ok depois manda ver com FLOR DA PELE bora com ...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11514</th>\n",
              "      <td>se eu fosse mãe dessa criatura mandava calar a...</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14232</th>\n",
              "      <td>de luto pela saída do USERNAME do</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>com o dr augusto cury falando sobre ansiedade ...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2250 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     doc  pol result\n",
              "1670     esse homem é uma perdição 😍 😈 USERNAME USERNAME    1    NaN\n",
              "13379     que fome da porra aí eu invento de ir assistir   -1    NaN\n",
              "10234                                      boa noite ! 📺    0    NaN\n",
              "4719                          A moça rindo 😱 😱 😱 😂 😂 😂 😂    1    NaN\n",
              "7003   USERNAME agora que o bicho vai pegar 😂 😂 😂 😂 😂...    0    NaN\n",
              "...                                                  ...  ...    ...\n",
              "3943   o filho do serginho ❤ ️ ❤ ️ ❤ ️ to apaixonadaa...    1    NaN\n",
              "3183   ok depois manda ver com FLOR DA PELE bora com ...    1    NaN\n",
              "11514  se eu fosse mãe dessa criatura mandava calar a...   -1    NaN\n",
              "14232                  de luto pela saída do USERNAME do   -1    NaN\n",
              "530    com o dr augusto cury falando sobre ansiedade ...    1    NaN\n",
              "\n",
              "[2250 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfnT-XRFYaya"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
        "f1 = f1_score(polarity, results, average = 'weighted')\n",
        "recall = recall_score(polarity, results, average = 'weighted')\n",
        "accuracy = accuracy_score(polarity, results)\n",
        "precision = precision_score(polarity, results, average = 'weighted')\n",
        "matrix = confusion_matrix(polarity, results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vPmmAhjbr3V"
      },
      "source": [
        "print(f1)\n",
        "print(recall)\n",
        "print(accuracy)\n",
        "print(precision)\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mR9inUgRJCR"
      },
      "source": [
        "## **5 - Conclusão**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2QoD8zCRZcQ"
      },
      "source": [
        "**Experimentos**\r\n",
        "*   Experimento A\r\n",
        " - Normalização, tokenização, remoção de\r\n",
        "stopwords e POS Tagging\r\n",
        "*   Experimento B\r\n",
        " - Acréscimo de lematização\r\n",
        "*   Experimento C\r\n",
        " - Acréscimo de correção ortográfica\r\n",
        "\r\n",
        "\r\n",
        "**Resultados**\r\n",
        "\r\n",
        "| Métricas | Experimento A | Experimento B | Experimento C |\r\n",
        "|---------|----------------|---------------|---------------|\r\n",
        "| Medida F | 0,544 | 0,580 | 0,578 |\r\n",
        "| Revocação | 0,539 | 0,584 | 0,585 |\r\n",
        "| Acurácia | 0,539 | 0,584 | 0,585 |\r\n",
        "| Precisão | 0,581 | 0,578 | 0,576 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Referências**\r\n",
        "*   Brum, H. B. and Nunes, M. d. G. V. (2017). Building a sentiment\r\n",
        "corpus of tweets in brazilian portuguese.CoRR, abs/1712.08917.\r\n",
        "*   Gonçalves, P., Araújo, M., Benevenuto, F., and Cha, M. (2013).\r\n",
        "Comparing and combining sentiment analysis methods.\r\n",
        "InProceedings of the First ACM Conference on Online Social\r\n",
        "Networks, COSN ’13, pages 27–38, New York, NY, USA. ACM.\r\n",
        "*   Zhao, J., Liu, K., and Xu, L. (2016). Book review: Sentiment\r\n",
        "analysis: Mining opinions, sentiments, and emotions by bing\r\n",
        "Liu.Computational Linguistics, 42(3):595–598"
      ]
    }
  ]
}